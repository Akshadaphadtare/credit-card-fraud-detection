{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset: https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "FILE_NAME = 'creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Label  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "full_data = pd.read_csv(FILE_NAME)\n",
    "\n",
    "#rename the 'Class' column\n",
    "full_data.rename(columns = {'Class': 'Label'}, inplace = True)\n",
    "\n",
    "#let's take a peek\n",
    "print full_data.shape\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data consists of 284807 instances of data with 29 total features with value counts of \n",
      "0    284315\n",
      "1       492\n",
      "Name: Label, dtype: int64\n",
      "Where 0 indicates a legitimate transaction and 1 indicates fraud\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "full_data = shuffle(full_data)\n",
    "\n",
    "# Seperate target labels\n",
    "labels = full_data['Label']\n",
    "\n",
    "times = full_data['Time']\n",
    "features = full_data.drop(['Time', 'Label'], axis=1)\n",
    "\n",
    "# Get some specifics on our dataset\n",
    "print \"Data consists of {} instances of data with {} total features with value counts of \\n{}\".format(\n",
    "    features.shape[0], features.shape[1], labels.value_counts())\n",
    "print \"Where 0 indicates a legitimate transaction and 1 indicates fraud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the amount spent\n",
    "features['normAmount'] = StandardScaler().fit_transform(features['Amount'].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235327</th>\n",
       "      <td>1.703300</td>\n",
       "      <td>-0.863266</td>\n",
       "      <td>0.104905</td>\n",
       "      <td>1.247179</td>\n",
       "      <td>-0.141436</td>\n",
       "      <td>2.679679</td>\n",
       "      <td>-1.520703</td>\n",
       "      <td>1.006440</td>\n",
       "      <td>1.831379</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381504</td>\n",
       "      <td>0.059823</td>\n",
       "      <td>0.558681</td>\n",
       "      <td>0.301081</td>\n",
       "      <td>-0.708090</td>\n",
       "      <td>-0.460214</td>\n",
       "      <td>-0.605743</td>\n",
       "      <td>0.138376</td>\n",
       "      <td>-0.039287</td>\n",
       "      <td>-0.265072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266360</th>\n",
       "      <td>1.964137</td>\n",
       "      <td>0.058253</td>\n",
       "      <td>-1.797970</td>\n",
       "      <td>0.581708</td>\n",
       "      <td>0.126463</td>\n",
       "      <td>-1.452885</td>\n",
       "      <td>0.293474</td>\n",
       "      <td>-0.357730</td>\n",
       "      <td>0.646047</td>\n",
       "      <td>-0.529761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149845</td>\n",
       "      <td>0.216898</td>\n",
       "      <td>0.691547</td>\n",
       "      <td>-0.048083</td>\n",
       "      <td>-0.095469</td>\n",
       "      <td>0.207807</td>\n",
       "      <td>-0.101422</td>\n",
       "      <td>-0.008903</td>\n",
       "      <td>-0.022834</td>\n",
       "      <td>-0.167318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35223</th>\n",
       "      <td>1.369382</td>\n",
       "      <td>-0.436701</td>\n",
       "      <td>-0.346418</td>\n",
       "      <td>-0.821032</td>\n",
       "      <td>-0.338874</td>\n",
       "      <td>-0.399493</td>\n",
       "      <td>-0.210304</td>\n",
       "      <td>-0.068217</td>\n",
       "      <td>-1.341384</td>\n",
       "      <td>0.871767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533924</td>\n",
       "      <td>-0.359787</td>\n",
       "      <td>-0.575358</td>\n",
       "      <td>-0.019121</td>\n",
       "      <td>-0.274605</td>\n",
       "      <td>0.358820</td>\n",
       "      <td>1.106498</td>\n",
       "      <td>-0.079808</td>\n",
       "      <td>-0.022458</td>\n",
       "      <td>-0.321245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152565</th>\n",
       "      <td>1.974238</td>\n",
       "      <td>-0.404622</td>\n",
       "      <td>0.217208</td>\n",
       "      <td>0.539687</td>\n",
       "      <td>-0.934828</td>\n",
       "      <td>-0.375689</td>\n",
       "      <td>-0.945837</td>\n",
       "      <td>-0.071938</td>\n",
       "      <td>2.622833</td>\n",
       "      <td>-0.401860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178805</td>\n",
       "      <td>-0.118053</td>\n",
       "      <td>0.070297</td>\n",
       "      <td>0.303308</td>\n",
       "      <td>-0.133564</td>\n",
       "      <td>-0.591890</td>\n",
       "      <td>0.447994</td>\n",
       "      <td>-0.037839</td>\n",
       "      <td>-0.043660</td>\n",
       "      <td>-0.289460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280472</th>\n",
       "      <td>1.713941</td>\n",
       "      <td>-1.532205</td>\n",
       "      <td>-1.721972</td>\n",
       "      <td>-0.184136</td>\n",
       "      <td>-0.715999</td>\n",
       "      <td>-0.587145</td>\n",
       "      <td>-0.178479</td>\n",
       "      <td>-0.177906</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.731916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283905</td>\n",
       "      <td>0.059015</td>\n",
       "      <td>0.165055</td>\n",
       "      <td>-0.128037</td>\n",
       "      <td>0.559865</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>-0.059637</td>\n",
       "      <td>-0.049411</td>\n",
       "      <td>-0.011774</td>\n",
       "      <td>0.626302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "235327  1.703300 -0.863266  0.104905  1.247179 -0.141436  2.679679 -1.520703   \n",
       "266360  1.964137  0.058253 -1.797970  0.581708  0.126463 -1.452885  0.293474   \n",
       "35223   1.369382 -0.436701 -0.346418 -0.821032 -0.338874 -0.399493 -0.210304   \n",
       "152565  1.974238 -0.404622  0.217208  0.539687 -0.934828 -0.375689 -0.945837   \n",
       "280472  1.713941 -1.532205 -1.721972 -0.184136 -0.715999 -0.587145 -0.178479   \n",
       "\n",
       "              V8        V9       V10     ...           V20       V21  \\\n",
       "235327  1.006440  1.831379 -0.026611     ...     -0.381504  0.059823   \n",
       "266360 -0.357730  0.646047 -0.529761     ...     -0.149845  0.216898   \n",
       "35223  -0.068217 -1.341384  0.871767     ...     -0.533924 -0.359787   \n",
       "152565 -0.071938  2.622833 -0.401860     ...     -0.178805 -0.118053   \n",
       "280472 -0.177906  0.032715  0.731916     ...     -0.283905  0.059015   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "235327  0.558681  0.301081 -0.708090 -0.460214 -0.605743  0.138376 -0.039287   \n",
       "266360  0.691547 -0.048083 -0.095469  0.207807 -0.101422 -0.008903 -0.022834   \n",
       "35223  -0.575358 -0.019121 -0.274605  0.358820  1.106498 -0.079808 -0.022458   \n",
       "152565  0.070297  0.303308 -0.133564 -0.591890  0.447994 -0.037839 -0.043660   \n",
       "280472  0.165055 -0.128037  0.559865  0.072100 -0.059637 -0.049411 -0.011774   \n",
       "\n",
       "        normAmount  \n",
       "235327   -0.265072  \n",
       "266360   -0.167318  \n",
       "35223    -0.321245  \n",
       "152565   -0.289460  \n",
       "280472    0.626302  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts = features['Amount']\n",
    "features = features.drop(['Amount'], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#?????????????????\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    213230\n",
      "1       375\n",
      "Name: Label, dtype: int64\n",
      "0    71085\n",
      "1      117\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label = 0)\n",
    "\n",
    "# We're going to hold out a test set from oversampling to see how our model trained on the oversampled data does on the original data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = .25)\n",
    "\n",
    "print train_labels.value_counts()\n",
    "print test_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# implement synthetic minority oversampling technique for a more balanced dataset to feed our model\n",
    "oversampler = SMOTE(random_state=331)\n",
    "os_features, os_labels = oversampler.fit_sample(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training instances of data: 319845\n",
      "training instances of fraud 159912\n",
      "testing instances of data: 106615\n",
      "testing instances of fraud: 53318\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(os_features, os_labels, test_size = .25)\n",
    "\n",
    "# Let's get an idea of what our new oversampled data looks like\n",
    "\n",
    "print 'training instances of data:' , len(y_train) \n",
    "print 'training instances of fraud' , np.count_nonzero(y_train)\n",
    "print 'testing instances of data:' , len(y_test) \n",
    "print 'testing instances of fraud:' , np.count_nonzero(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Random Forest Classifier:\n",
      "[ 0.99981707  0.99982411  0.99983818] 0.999826453198\n",
      "For K-Nearest Neighbors Classifier:\n",
      "[ 0.99896484  0.99868985  0.99888023] 0.998844970617\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "print \"For Random Forest Classifier:\"\n",
    "rfscores = cross_val_score(rf, os_features, os_labels, scoring = f1_scorer)\n",
    "print rfscores, rfscores.mean()\n",
    "\n",
    "print \"For K-Nearest Neighbors Classifier:\"\n",
    "knnscores = cross_val_score(knn, os_features, os_labels, scoring = f1_scorer)\n",
    "print knnscores, knnscores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for simple majority vote is  0.999135510488\n"
     ]
    }
   ],
   "source": [
    "#majority vote benchmark without oversampling\n",
    "majority_vote_predictions = np.zeros(features.shape[0])\n",
    "print \"f1 score for simple majority vote is \" , f1_score(labels, majority_vote_predictions, pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[CV] n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1, score=0.999866 -   0.6s\n",
      "[CV] n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1, score=0.999831 -   0.6s\n",
      "[CV] n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   51.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=25, min_samples_split=2, criterion=entropy, max_features=1, score=0.999845 -   0.5s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21, score=0.999676 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21, score=0.999627 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=21, score=0.999789 -   0.1s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=9, score=0.999824 -   1.8s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=9, score=0.999838 -   1.9s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=9, score=0.999824 -   1.8s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=9, score=0.999824 -   1.5s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=9, score=0.999845 -   1.5s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=9 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=9, score=0.999859 -   1.5s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=13, score=0.999796 -   2.5s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=13, score=0.999817 -   2.7s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=gini, max_features=13 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=gini, max_features=13, score=0.999824 -   2.3s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17, score=0.999810 - 1.7min\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17, score=0.999838 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=entropy, max_features=17, score=0.999859 -   1.9s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=21, score=0.999655 -   1.1s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=21, score=0.999733 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=gini, max_features=21 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=gini, max_features=21, score=0.999740 -   1.2s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=5, score=0.999838 -   1.7s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=5, score=0.999845 -   1.7s\n",
      "[CV] n_estimators=70, min_samples_split=2, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=2, criterion=gini, max_features=5, score=0.999845 -   1.5s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=gini, max_features=1, score=0.999873 -   1.8s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=gini, max_features=1, score=0.999824 -   1.8s\n",
      "[CV] n_estimators=70, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=70, min_samples_split=4, criterion=gini, max_features=1, score=0.999831 -   1.8s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5, score=0.999852 -   1.2s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5, score=0.999845 -   1.3s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=5, score=0.999838 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5, score=0.999845 -   1.0s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5, score=0.999838 -   1.0s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=5, score=0.999824 -   1.0s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9, score=0.999838 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9, score=0.999831 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=10, min_samples_split=2, criterion=entropy, max_features=9, score=0.999796 -   0.1s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=5, score=0.999817 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=5, score=0.999852 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=55, min_samples_split=6, criterion=gini, max_features=5, score=0.999824 -   1.1s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25, score=0.999683 -   1.6s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25, score=0.999634 -   2.5s\n",
      "[CV] n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25 \n",
      "[CV]  n_estimators=100, min_samples_split=6, criterion=entropy, max_features=25, score=0.999627 -   1.6s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=1, score=0.999866 -   2.3s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=1, score=0.999845 -   2.4s\n",
      "[CV] n_estimators=100, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=4, criterion=gini, max_features=1, score=0.999859 -   2.4s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1, score=0.999859 -   2.2s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1, score=0.999852 -   2.3s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=1, score=0.999859 -   2.3s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=1, score=0.999817 -   0.2s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=1, score=0.999789 -   0.2s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=1 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=1, score=0.999845 -   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9, score=0.999817 -   1.7s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9, score=0.999824 -   1.7s\n",
      "[CV] n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9 \n",
      "[CV]  n_estimators=100, min_samples_split=2, criterion=entropy, max_features=9, score=0.999845 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=5, score=0.999838 -   1.8s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=5, score=0.999845 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=gini, max_features=5 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=gini, max_features=5, score=0.999824 -   1.8s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1, score=0.999873 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1, score=0.999810 -   1.2s\n",
      "[CV] n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=55, min_samples_split=2, criterion=entropy, max_features=1, score=0.999845 -   1.2s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13, score=0.999845 -   1.4s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13, score=0.999817 -   1.5s\n",
      "[CV] n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13 \n",
      "[CV]  n_estimators=85, min_samples_split=4, criterion=entropy, max_features=13, score=0.999859 -   1.4s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1, score=0.999852 -   1.9s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1, score=0.999838 -   1.9s\n",
      "[CV] n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1 \n",
      "[CV]  n_estimators=85, min_samples_split=6, criterion=entropy, max_features=1, score=0.999852 -   1.8s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=17, score=0.999669 -   0.1s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=17, score=0.999719 -   0.2s\n",
      "[CV] n_estimators=10, min_samples_split=4, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=10, min_samples_split=4, criterion=gini, max_features=17, score=0.999719 -   0.1s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21, score=0.999754 -   1.1s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21, score=0.999796 -   1.1s\n",
      "[CV] n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21 \n",
      "[CV]  n_estimators=70, min_samples_split=6, criterion=entropy, max_features=21, score=0.999768 -   1.1s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=gini, max_features=17, score=0.999754 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=gini, max_features=17, score=0.999824 -   1.7s\n",
      "[CV] n_estimators=85, min_samples_split=2, criterion=gini, max_features=17 \n",
      "[CV]  n_estimators=85, min_samples_split=2, criterion=gini, max_features=17, score=0.999775 -   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 439.6min finished\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'n_estimators' : np.arange(10, 110, 15),\n",
    "                'min_samples_split': np.arange(2, 8, 2),\n",
    "                'max_features': np.arange(1, 29, 4),\n",
    "                'criterion': ['gini', 'entropy']}\n",
    "\n",
    "\n",
    "rf_tune = RandomizedSearchCV(rf, rf_params, n_iter = 25, scoring = f1_scorer, verbose = 3)\n",
    "\n",
    "rf_tune = rf_tune.fit(os_features, os_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features=1, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False) \n",
      "f1 score: 0.999856941492\n"
     ]
    }
   ],
   "source": [
    "print rf_tune.best_estimator_ , '\\nf1 score:' , rf_tune.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\Sunny\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.14748263, -0.41471575, -0.27715551, -0.62149895,  0.361638  ,\n",
       "         0.04151148],\n",
       "       [-0.78123839, -0.96877863, -0.57963475, -1.3623686 ,  0.11400963,\n",
       "        -0.3206355 ],\n",
       "       [-6.27852821,  3.01893411, -6.60179368, -7.68470916, -2.57070958,\n",
       "         0.918958  ],\n",
       "       ..., \n",
       "       [ 0.1026754 ,  1.05104284,  1.64328036, -0.30613186,  0.36443625,\n",
       "         0.14194996],\n",
       "       [ 0.03093379,  0.49913424, -0.08120353,  0.76550463,  0.39444543,\n",
       "         2.08177092],\n",
       "       [-0.31799603, -0.60027605,  0.63097503, -0.58918361, -0.18149779,\n",
       "        -0.32864283]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft = rf_tune.best_estimator_\n",
    "rft.fit_transform(X_train, y_train)\n",
    "\n",
    "rfu = RandomForestClassifier()\n",
    "rfu.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 testing score for tuned random forest on oversampled data is  0.999849875209\n",
      "f1 testing score for random forest on oversampled data is  0.999812344011\n",
      "f1 testing score for tuned random forest on original test data is  0.999746789146\n"
     ]
    }
   ],
   "source": [
    "# Check performances of tuned and untuned models \n",
    "# Add check on original data before oversampling\n",
    "print \"f1 testing score for tuned random forest on oversampled data is \", f1_score(y_test, rft.predict(X_test), pos_label = 0)\n",
    "print \"f1 testing score for random forest on oversampled data is \" , f1_score(y_test, rfu.predict(X_test), pos_label = 0)\n",
    "\n",
    "print \"f1 testing score for tuned random forest on original test data is \", f1_score(test_labels, rft.predict(test_features), pos_label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53281,    16],\n",
       "       [    0, 53318]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rft.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now let's try a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's build a model\n",
    "\n",
    "# needs more tuning\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim = X_train.shape[1], activation = 'tanh', init = 'lecun_uniform', W_regularizer = l2(.01)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(12, activation = 'tanh', init = 'lecun_uniform'))\n",
    "model.add(Dense(4, activation = 'tanh', init = 'lecun_uniform'))\n",
    "model.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "sgd = SGD(lr = .16, momentum = .7, decay = .01)\n",
    "\n",
    "model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 255876 samples, validate on 63969 samples\n",
      "Epoch 1/256\n",
      "1s - loss: 0.1574 - fmeasure: 0.9574 - val_loss: 0.0675 - val_fmeasure: 0.9725\n",
      "Epoch 2/256\n",
      "1s - loss: 0.0798 - fmeasure: 0.9789 - val_loss: 0.0439 - val_fmeasure: 0.9824\n",
      "Epoch 3/256\n",
      "1s - loss: 0.0655 - fmeasure: 0.9849 - val_loss: 0.0361 - val_fmeasure: 0.9873\n",
      "Epoch 4/256\n",
      "1s - loss: 0.0569 - fmeasure: 0.9881 - val_loss: 0.0300 - val_fmeasure: 0.9898\n",
      "Epoch 5/256\n",
      "1s - loss: 0.0519 - fmeasure: 0.9896 - val_loss: 0.0760 - val_fmeasure: 0.9672\n",
      "Epoch 6/256\n",
      "1s - loss: 0.0483 - fmeasure: 0.9904 - val_loss: 0.0226 - val_fmeasure: 0.9935\n",
      "Epoch 7/256\n",
      "1s - loss: 0.0457 - fmeasure: 0.9918 - val_loss: 0.0302 - val_fmeasure: 0.9894\n",
      "Epoch 8/256\n",
      "1s - loss: 0.0434 - fmeasure: 0.9922 - val_loss: 0.0243 - val_fmeasure: 0.9921\n",
      "Epoch 9/256\n",
      "1s - loss: 0.0408 - fmeasure: 0.9929 - val_loss: 0.0207 - val_fmeasure: 0.9942\n",
      "Epoch 10/256\n",
      "1s - loss: 0.0379 - fmeasure: 0.9935 - val_loss: 0.0200 - val_fmeasure: 0.9935\n",
      "Epoch 11/256\n",
      "2s - loss: 0.0375 - fmeasure: 0.9937 - val_loss: 0.0213 - val_fmeasure: 0.9942\n",
      "Epoch 12/256\n",
      "1s - loss: 0.0354 - fmeasure: 0.9941 - val_loss: 0.0177 - val_fmeasure: 0.9942\n",
      "Epoch 13/256\n",
      "1s - loss: 0.0341 - fmeasure: 0.9944 - val_loss: 0.0193 - val_fmeasure: 0.9942\n",
      "Epoch 14/256\n",
      "1s - loss: 0.0340 - fmeasure: 0.9943 - val_loss: 0.0169 - val_fmeasure: 0.9944\n",
      "Epoch 15/256\n",
      "1s - loss: 0.0331 - fmeasure: 0.9948 - val_loss: 0.0171 - val_fmeasure: 0.9948\n",
      "Epoch 16/256\n",
      "1s - loss: 0.0318 - fmeasure: 0.9950 - val_loss: 0.0160 - val_fmeasure: 0.9956\n",
      "Epoch 17/256\n",
      "1s - loss: 0.0320 - fmeasure: 0.9947 - val_loss: 0.0168 - val_fmeasure: 0.9960\n",
      "Epoch 18/256\n",
      "1s - loss: 0.0328 - fmeasure: 0.9946 - val_loss: 0.0158 - val_fmeasure: 0.9958\n",
      "Epoch 19/256\n",
      "1s - loss: 0.0306 - fmeasure: 0.9953 - val_loss: 0.0171 - val_fmeasure: 0.9954\n",
      "Epoch 20/256\n",
      "1s - loss: 0.0300 - fmeasure: 0.9954 - val_loss: 0.0162 - val_fmeasure: 0.9953\n",
      "Epoch 21/256\n",
      "1s - loss: 0.0295 - fmeasure: 0.9953 - val_loss: 0.0157 - val_fmeasure: 0.9951\n",
      "Epoch 22/256\n",
      "1s - loss: 0.0289 - fmeasure: 0.9955 - val_loss: 0.0148 - val_fmeasure: 0.9963\n",
      "Epoch 23/256\n",
      "1s - loss: 0.0290 - fmeasure: 0.9955 - val_loss: 0.0152 - val_fmeasure: 0.9954\n",
      "Epoch 24/256\n",
      "1s - loss: 0.0283 - fmeasure: 0.9957 - val_loss: 0.0152 - val_fmeasure: 0.9952\n",
      "Epoch 25/256\n",
      "1s - loss: 0.0282 - fmeasure: 0.9956 - val_loss: 0.0185 - val_fmeasure: 0.9943\n",
      "Epoch 26/256\n",
      "1s - loss: 0.0285 - fmeasure: 0.9958 - val_loss: 0.0147 - val_fmeasure: 0.9955\n",
      "Epoch 27/256\n",
      "1s - loss: 0.0272 - fmeasure: 0.9961 - val_loss: 0.0150 - val_fmeasure: 0.9962\n",
      "Epoch 28/256\n",
      "1s - loss: 0.0268 - fmeasure: 0.9960 - val_loss: 0.0149 - val_fmeasure: 0.9960\n",
      "Epoch 29/256\n",
      "1s - loss: 0.0271 - fmeasure: 0.9958 - val_loss: 0.0149 - val_fmeasure: 0.9957\n",
      "Epoch 30/256\n",
      "1s - loss: 0.0266 - fmeasure: 0.9960 - val_loss: 0.0139 - val_fmeasure: 0.9963\n",
      "Epoch 31/256\n",
      "1s - loss: 0.0262 - fmeasure: 0.9960 - val_loss: 0.0146 - val_fmeasure: 0.9958\n",
      "Epoch 32/256\n",
      "1s - loss: 0.0264 - fmeasure: 0.9959 - val_loss: 0.0142 - val_fmeasure: 0.9972\n",
      "Epoch 33/256\n",
      "1s - loss: 0.0264 - fmeasure: 0.9960 - val_loss: 0.0137 - val_fmeasure: 0.9966\n",
      "Epoch 34/256\n",
      "1s - loss: 0.0263 - fmeasure: 0.9959 - val_loss: 0.0163 - val_fmeasure: 0.9951\n",
      "Epoch 35/256\n",
      "1s - loss: 0.0255 - fmeasure: 0.9962 - val_loss: 0.0200 - val_fmeasure: 0.9936\n",
      "Epoch 36/256\n",
      "1s - loss: 0.0258 - fmeasure: 0.9961 - val_loss: 0.0131 - val_fmeasure: 0.9971\n",
      "Epoch 37/256\n",
      "1s - loss: 0.0254 - fmeasure: 0.9962 - val_loss: 0.0213 - val_fmeasure: 0.9934\n",
      "Epoch 38/256\n",
      "1s - loss: 0.0252 - fmeasure: 0.9963 - val_loss: 0.0126 - val_fmeasure: 0.9972\n",
      "Epoch 39/256\n",
      "1s - loss: 0.0249 - fmeasure: 0.9964 - val_loss: 0.0159 - val_fmeasure: 0.9959\n",
      "Epoch 40/256\n",
      "1s - loss: 0.0252 - fmeasure: 0.9962 - val_loss: 0.0130 - val_fmeasure: 0.9966\n",
      "Epoch 41/256\n",
      "1s - loss: 0.0249 - fmeasure: 0.9962 - val_loss: 0.0128 - val_fmeasure: 0.9967\n",
      "Epoch 42/256\n",
      "1s - loss: 0.0247 - fmeasure: 0.9964 - val_loss: 0.0151 - val_fmeasure: 0.9952\n",
      "Epoch 43/256\n",
      "1s - loss: 0.0248 - fmeasure: 0.9963 - val_loss: 0.0124 - val_fmeasure: 0.9969\n",
      "Epoch 44/256\n",
      "1s - loss: 0.0245 - fmeasure: 0.9963 - val_loss: 0.0146 - val_fmeasure: 0.9963\n",
      "Epoch 45/256\n",
      "1s - loss: 0.0240 - fmeasure: 0.9966 - val_loss: 0.0184 - val_fmeasure: 0.9955\n",
      "Epoch 46/256\n",
      "1s - loss: 0.0243 - fmeasure: 0.9963 - val_loss: 0.0123 - val_fmeasure: 0.9974\n",
      "Epoch 47/256\n",
      "1s - loss: 0.0238 - fmeasure: 0.9965 - val_loss: 0.0130 - val_fmeasure: 0.9973\n",
      "Epoch 48/256\n",
      "1s - loss: 0.0236 - fmeasure: 0.9966 - val_loss: 0.0122 - val_fmeasure: 0.9967\n",
      "Epoch 49/256\n",
      "1s - loss: 0.0235 - fmeasure: 0.9965 - val_loss: 0.0124 - val_fmeasure: 0.9964\n",
      "Epoch 50/256\n",
      "1s - loss: 0.0236 - fmeasure: 0.9966 - val_loss: 0.0120 - val_fmeasure: 0.9973\n",
      "Epoch 51/256\n",
      "1s - loss: 0.0232 - fmeasure: 0.9968 - val_loss: 0.0117 - val_fmeasure: 0.9972\n",
      "Epoch 52/256\n",
      "1s - loss: 0.0230 - fmeasure: 0.9967 - val_loss: 0.0127 - val_fmeasure: 0.9964\n",
      "Epoch 53/256\n",
      "1s - loss: 0.0228 - fmeasure: 0.9967 - val_loss: 0.0121 - val_fmeasure: 0.9974\n",
      "Epoch 54/256\n",
      "1s - loss: 0.0230 - fmeasure: 0.9968 - val_loss: 0.0120 - val_fmeasure: 0.9972\n",
      "Epoch 55/256\n",
      "2s - loss: 0.0229 - fmeasure: 0.9967 - val_loss: 0.0122 - val_fmeasure: 0.9970\n",
      "Epoch 56/256\n",
      "2s - loss: 0.0228 - fmeasure: 0.9967 - val_loss: 0.0124 - val_fmeasure: 0.9969\n",
      "Epoch 57/256\n",
      "1s - loss: 0.0230 - fmeasure: 0.9967 - val_loss: 0.0122 - val_fmeasure: 0.9967\n",
      "Epoch 58/256\n",
      "1s - loss: 0.0227 - fmeasure: 0.9969 - val_loss: 0.0131 - val_fmeasure: 0.9969\n",
      "Epoch 59/256\n",
      "1s - loss: 0.0226 - fmeasure: 0.9969 - val_loss: 0.0140 - val_fmeasure: 0.9960\n",
      "Epoch 60/256\n",
      "1s - loss: 0.0229 - fmeasure: 0.9967 - val_loss: 0.0137 - val_fmeasure: 0.9970\n",
      "Epoch 61/256\n",
      "1s - loss: 0.0223 - fmeasure: 0.9968 - val_loss: 0.0112 - val_fmeasure: 0.9975\n",
      "Epoch 62/256\n",
      "1s - loss: 0.0226 - fmeasure: 0.9968 - val_loss: 0.0116 - val_fmeasure: 0.9970\n",
      "Epoch 63/256\n",
      "1s - loss: 0.0223 - fmeasure: 0.9969 - val_loss: 0.0116 - val_fmeasure: 0.9968\n",
      "Epoch 64/256\n",
      "1s - loss: 0.0222 - fmeasure: 0.9969 - val_loss: 0.0134 - val_fmeasure: 0.9969\n",
      "Epoch 65/256\n",
      "1s - loss: 0.0218 - fmeasure: 0.9970 - val_loss: 0.0123 - val_fmeasure: 0.9964\n",
      "Epoch 66/256\n",
      "1s - loss: 0.0221 - fmeasure: 0.9969 - val_loss: 0.0118 - val_fmeasure: 0.9971\n",
      "Epoch 67/256\n",
      "1s - loss: 0.0218 - fmeasure: 0.9971 - val_loss: 0.0110 - val_fmeasure: 0.9975\n",
      "Epoch 68/256\n",
      "1s - loss: 0.0223 - fmeasure: 0.9969 - val_loss: 0.0120 - val_fmeasure: 0.9970\n",
      "Epoch 69/256\n",
      "1s - loss: 0.0218 - fmeasure: 0.9970 - val_loss: 0.0119 - val_fmeasure: 0.9971\n",
      "Epoch 70/256\n",
      "1s - loss: 0.0218 - fmeasure: 0.9970 - val_loss: 0.0126 - val_fmeasure: 0.9970\n",
      "Epoch 71/256\n",
      "1s - loss: 0.0217 - fmeasure: 0.9969 - val_loss: 0.0126 - val_fmeasure: 0.9970\n",
      "Epoch 72/256\n",
      "1s - loss: 0.0215 - fmeasure: 0.9970 - val_loss: 0.0120 - val_fmeasure: 0.9968\n",
      "Epoch 73/256\n",
      "1s - loss: 0.0216 - fmeasure: 0.9970 - val_loss: 0.0116 - val_fmeasure: 0.9973\n",
      "Epoch 74/256\n",
      "1s - loss: 0.0216 - fmeasure: 0.9971 - val_loss: 0.0111 - val_fmeasure: 0.9972\n",
      "Epoch 75/256\n",
      "1s - loss: 0.0215 - fmeasure: 0.9971 - val_loss: 0.0114 - val_fmeasure: 0.9971\n",
      "Epoch 76/256\n",
      "1s - loss: 0.0214 - fmeasure: 0.9971 - val_loss: 0.0114 - val_fmeasure: 0.9975\n",
      "Epoch 77/256\n",
      "1s - loss: 0.0212 - fmeasure: 0.9972 - val_loss: 0.0124 - val_fmeasure: 0.9971\n",
      "Epoch 78/256\n",
      "1s - loss: 0.0214 - fmeasure: 0.9970 - val_loss: 0.0123 - val_fmeasure: 0.9965\n",
      "Epoch 79/256\n",
      "1s - loss: 0.0212 - fmeasure: 0.9970 - val_loss: 0.0138 - val_fmeasure: 0.9967\n",
      "Epoch 80/256\n",
      "1s - loss: 0.0213 - fmeasure: 0.9970 - val_loss: 0.0116 - val_fmeasure: 0.9967\n",
      "Epoch 81/256\n",
      "1s - loss: 0.0211 - fmeasure: 0.9971 - val_loss: 0.0116 - val_fmeasure: 0.9968\n",
      "Epoch 82/256\n",
      "1s - loss: 0.0210 - fmeasure: 0.9972 - val_loss: 0.0116 - val_fmeasure: 0.9970\n",
      "Epoch 83/256\n",
      "1s - loss: 0.0207 - fmeasure: 0.9972 - val_loss: 0.0120 - val_fmeasure: 0.9969\n",
      "Epoch 84/256\n",
      "1s - loss: 0.0211 - fmeasure: 0.9971 - val_loss: 0.0110 - val_fmeasure: 0.9976\n",
      "Epoch 85/256\n",
      "1s - loss: 0.0207 - fmeasure: 0.9971 - val_loss: 0.0115 - val_fmeasure: 0.9976\n",
      "Epoch 86/256\n",
      "1s - loss: 0.0210 - fmeasure: 0.9971 - val_loss: 0.0112 - val_fmeasure: 0.9972\n",
      "Epoch 87/256\n",
      "1s - loss: 0.0206 - fmeasure: 0.9973 - val_loss: 0.0116 - val_fmeasure: 0.9965\n",
      "Epoch 88/256\n",
      "1s - loss: 0.0206 - fmeasure: 0.9971 - val_loss: 0.0111 - val_fmeasure: 0.9974\n",
      "Epoch 89/256\n",
      "1s - loss: 0.0206 - fmeasure: 0.9973 - val_loss: 0.0107 - val_fmeasure: 0.9973\n",
      "Epoch 90/256\n",
      "1s - loss: 0.0205 - fmeasure: 0.9972 - val_loss: 0.0124 - val_fmeasure: 0.9972\n",
      "Epoch 91/256\n",
      "1s - loss: 0.0206 - fmeasure: 0.9972 - val_loss: 0.0111 - val_fmeasure: 0.9975\n",
      "Epoch 92/256\n",
      "1s - loss: 0.0204 - fmeasure: 0.9973 - val_loss: 0.0156 - val_fmeasure: 0.9964\n",
      "Epoch 93/256\n",
      "1s - loss: 0.0206 - fmeasure: 0.9971 - val_loss: 0.0115 - val_fmeasure: 0.9973\n",
      "Epoch 94/256\n",
      "1s - loss: 0.0216 - fmeasure: 0.9969 - val_loss: 0.0120 - val_fmeasure: 0.9966\n",
      "Epoch 95/256\n",
      "1s - loss: 0.0207 - fmeasure: 0.9973 - val_loss: 0.0121 - val_fmeasure: 0.9965\n",
      "Epoch 96/256\n",
      "1s - loss: 0.0203 - fmeasure: 0.9973 - val_loss: 0.0112 - val_fmeasure: 0.9972\n",
      "Epoch 97/256\n",
      "1s - loss: 0.0206 - fmeasure: 0.9972 - val_loss: 0.0116 - val_fmeasure: 0.9978\n",
      "Epoch 98/256\n",
      "1s - loss: 0.0203 - fmeasure: 0.9973 - val_loss: 0.0113 - val_fmeasure: 0.9973\n",
      "Epoch 99/256\n",
      "1s - loss: 0.0202 - fmeasure: 0.9973 - val_loss: 0.0113 - val_fmeasure: 0.9976\n",
      "Epoch 100/256\n",
      "1s - loss: 0.0204 - fmeasure: 0.9971 - val_loss: 0.0114 - val_fmeasure: 0.9971\n",
      "Epoch 101/256\n",
      "1s - loss: 0.0202 - fmeasure: 0.9973 - val_loss: 0.0110 - val_fmeasure: 0.9972\n",
      "Epoch 102/256\n",
      "1s - loss: 0.0200 - fmeasure: 0.9973 - val_loss: 0.0121 - val_fmeasure: 0.9972\n",
      "Epoch 103/256\n",
      "1s - loss: 0.0203 - fmeasure: 0.9974 - val_loss: 0.0114 - val_fmeasure: 0.9975\n",
      "Epoch 104/256\n",
      "1s - loss: 0.0200 - fmeasure: 0.9973 - val_loss: 0.0107 - val_fmeasure: 0.9974\n",
      "Epoch 105/256\n",
      "1s - loss: 0.0199 - fmeasure: 0.9973 - val_loss: 0.0123 - val_fmeasure: 0.9968\n",
      "Epoch 106/256\n",
      "1s - loss: 0.0200 - fmeasure: 0.9974 - val_loss: 0.0138 - val_fmeasure: 0.9953\n",
      "Epoch 107/256\n",
      "1s - loss: 0.0200 - fmeasure: 0.9973 - val_loss: 0.0111 - val_fmeasure: 0.9971\n",
      "Epoch 108/256\n",
      "1s - loss: 0.0200 - fmeasure: 0.9973 - val_loss: 0.0119 - val_fmeasure: 0.9969\n",
      "Epoch 109/256\n",
      "1s - loss: 0.0198 - fmeasure: 0.9974 - val_loss: 0.0105 - val_fmeasure: 0.9976\n",
      "Epoch 110/256\n",
      "1s - loss: 0.0196 - fmeasure: 0.9975 - val_loss: 0.0114 - val_fmeasure: 0.9968\n",
      "Epoch 111/256\n",
      "1s - loss: 0.0200 - fmeasure: 0.9972 - val_loss: 0.0111 - val_fmeasure: 0.9971\n",
      "Epoch 112/256\n",
      "1s - loss: 0.0197 - fmeasure: 0.9974 - val_loss: 0.0111 - val_fmeasure: 0.9976\n",
      "Epoch 113/256\n",
      "1s - loss: 0.0197 - fmeasure: 0.9975 - val_loss: 0.0108 - val_fmeasure: 0.9972\n",
      "Epoch 114/256\n",
      "1s - loss: 0.0196 - fmeasure: 0.9974 - val_loss: 0.0105 - val_fmeasure: 0.9976\n",
      "Epoch 115/256\n",
      "1s - loss: 0.0195 - fmeasure: 0.9975 - val_loss: 0.0109 - val_fmeasure: 0.9971\n",
      "Epoch 116/256\n",
      "1s - loss: 0.0197 - fmeasure: 0.9974 - val_loss: 0.0112 - val_fmeasure: 0.9977\n",
      "Epoch 117/256\n",
      "1s - loss: 0.0194 - fmeasure: 0.9975 - val_loss: 0.0107 - val_fmeasure: 0.9979\n",
      "Epoch 118/256\n",
      "1s - loss: 0.0194 - fmeasure: 0.9975 - val_loss: 0.0110 - val_fmeasure: 0.9972\n",
      "Epoch 119/256\n",
      "1s - loss: 0.0195 - fmeasure: 0.9974 - val_loss: 0.0112 - val_fmeasure: 0.9977\n",
      "Epoch 120/256\n",
      "1s - loss: 0.0195 - fmeasure: 0.9975 - val_loss: 0.0111 - val_fmeasure: 0.9974\n",
      "Epoch 121/256\n",
      "1s - loss: 0.0193 - fmeasure: 0.9975 - val_loss: 0.0104 - val_fmeasure: 0.9980\n",
      "Epoch 122/256\n",
      "1s - loss: 0.0193 - fmeasure: 0.9974 - val_loss: 0.0107 - val_fmeasure: 0.9975\n",
      "Epoch 123/256\n",
      "1s - loss: 0.0193 - fmeasure: 0.9975 - val_loss: 0.0113 - val_fmeasure: 0.9969\n",
      "Epoch 124/256\n",
      "1s - loss: 0.0192 - fmeasure: 0.9976 - val_loss: 0.0108 - val_fmeasure: 0.9972\n",
      "Epoch 125/256\n",
      "1s - loss: 0.0194 - fmeasure: 0.9975 - val_loss: 0.0112 - val_fmeasure: 0.9977\n",
      "Epoch 126/256\n",
      "1s - loss: 0.0193 - fmeasure: 0.9975 - val_loss: 0.0105 - val_fmeasure: 0.9975\n",
      "Epoch 127/256\n",
      "1s - loss: 0.0190 - fmeasure: 0.9976 - val_loss: 0.0110 - val_fmeasure: 0.9974\n",
      "Epoch 128/256\n",
      "2s - loss: 0.0190 - fmeasure: 0.9975 - val_loss: 0.0106 - val_fmeasure: 0.9977\n",
      "Epoch 129/256\n",
      "1s - loss: 0.0191 - fmeasure: 0.9974 - val_loss: 0.0108 - val_fmeasure: 0.9976\n",
      "Epoch 130/256\n",
      "1s - loss: 0.0190 - fmeasure: 0.9976 - val_loss: 0.0104 - val_fmeasure: 0.9978\n",
      "Epoch 131/256\n",
      "1s - loss: 0.0190 - fmeasure: 0.9975 - val_loss: 0.0103 - val_fmeasure: 0.9974\n",
      "Epoch 132/256\n",
      "1s - loss: 0.0192 - fmeasure: 0.9973 - val_loss: 0.0110 - val_fmeasure: 0.9977\n",
      "Epoch 133/256\n",
      "1s - loss: 0.0191 - fmeasure: 0.9975 - val_loss: 0.0107 - val_fmeasure: 0.9970\n",
      "Epoch 134/256\n",
      "1s - loss: 0.0191 - fmeasure: 0.9975 - val_loss: 0.0109 - val_fmeasure: 0.9975\n",
      "Epoch 135/256\n",
      "1s - loss: 0.0190 - fmeasure: 0.9975 - val_loss: 0.0106 - val_fmeasure: 0.9977\n",
      "Epoch 136/256\n",
      "1s - loss: 0.0190 - fmeasure: 0.9975 - val_loss: 0.0103 - val_fmeasure: 0.9976\n",
      "Epoch 137/256\n",
      "1s - loss: 0.0188 - fmeasure: 0.9976 - val_loss: 0.0106 - val_fmeasure: 0.9976\n",
      "Epoch 138/256\n",
      "1s - loss: 0.0189 - fmeasure: 0.9975 - val_loss: 0.0113 - val_fmeasure: 0.9975\n",
      "Epoch 139/256\n",
      "1s - loss: 0.0189 - fmeasure: 0.9974 - val_loss: 0.0107 - val_fmeasure: 0.9975\n",
      "Epoch 140/256\n",
      "1s - loss: 0.0188 - fmeasure: 0.9976 - val_loss: 0.0109 - val_fmeasure: 0.9978\n",
      "Epoch 141/256\n",
      "1s - loss: 0.0188 - fmeasure: 0.9976 - val_loss: 0.0105 - val_fmeasure: 0.9977\n",
      "Epoch 142/256\n",
      "1s - loss: 0.0190 - fmeasure: 0.9976 - val_loss: 0.0114 - val_fmeasure: 0.9971\n",
      "Epoch 143/256\n",
      "1s - loss: 0.0186 - fmeasure: 0.9976 - val_loss: 0.0105 - val_fmeasure: 0.9974\n",
      "Epoch 144/256\n",
      "1s - loss: 0.0186 - fmeasure: 0.9977 - val_loss: 0.0111 - val_fmeasure: 0.9970\n",
      "Epoch 145/256\n",
      "1s - loss: 0.0187 - fmeasure: 0.9975 - val_loss: 0.0108 - val_fmeasure: 0.9977\n",
      "Epoch 146/256\n",
      "1s - loss: 0.0186 - fmeasure: 0.9976 - val_loss: 0.0105 - val_fmeasure: 0.9979\n",
      "Epoch 147/256\n",
      "1s - loss: 0.0186 - fmeasure: 0.9975 - val_loss: 0.0103 - val_fmeasure: 0.9979\n",
      "Epoch 148/256\n",
      "1s - loss: 0.0187 - fmeasure: 0.9975 - val_loss: 0.0107 - val_fmeasure: 0.9979\n",
      "Epoch 149/256\n",
      "1s - loss: 0.0186 - fmeasure: 0.9975 - val_loss: 0.0107 - val_fmeasure: 0.9973\n",
      "Epoch 150/256\n",
      "1s - loss: 0.0184 - fmeasure: 0.9976 - val_loss: 0.0101 - val_fmeasure: 0.9978\n",
      "Epoch 151/256\n",
      "1s - loss: 0.0185 - fmeasure: 0.9976 - val_loss: 0.0106 - val_fmeasure: 0.9975\n",
      "Epoch 152/256\n",
      "1s - loss: 0.0185 - fmeasure: 0.9976 - val_loss: 0.0109 - val_fmeasure: 0.9970\n",
      "Epoch 153/256\n",
      "1s - loss: 0.0185 - fmeasure: 0.9976 - val_loss: 0.0108 - val_fmeasure: 0.9976\n",
      "Epoch 154/256\n",
      "1s - loss: 0.0186 - fmeasure: 0.9977 - val_loss: 0.0111 - val_fmeasure: 0.9976\n",
      "Epoch 155/256\n",
      "1s - loss: 0.0183 - fmeasure: 0.9977 - val_loss: 0.0116 - val_fmeasure: 0.9967\n",
      "Epoch 156/256\n",
      "1s - loss: 0.0185 - fmeasure: 0.9977 - val_loss: 0.0102 - val_fmeasure: 0.9976\n",
      "Epoch 157/256\n",
      "1s - loss: 0.0185 - fmeasure: 0.9977 - val_loss: 0.0112 - val_fmeasure: 0.9968\n",
      "Epoch 158/256\n",
      "1s - loss: 0.0185 - fmeasure: 0.9976 - val_loss: 0.0107 - val_fmeasure: 0.9975\n",
      "Epoch 159/256\n",
      "1s - loss: 0.0185 - fmeasure: 0.9977 - val_loss: 0.0104 - val_fmeasure: 0.9975\n",
      "Epoch 160/256\n",
      "1s - loss: 0.0182 - fmeasure: 0.9977 - val_loss: 0.0103 - val_fmeasure: 0.9975\n",
      "Epoch 161/256\n",
      "1s - loss: 0.0184 - fmeasure: 0.9976 - val_loss: 0.0107 - val_fmeasure: 0.9976\n",
      "Epoch 162/256\n",
      "1s - loss: 0.0183 - fmeasure: 0.9976 - val_loss: 0.0103 - val_fmeasure: 0.9979\n",
      "Epoch 163/256\n",
      "1s - loss: 0.0182 - fmeasure: 0.9977 - val_loss: 0.0105 - val_fmeasure: 0.9972\n",
      "Epoch 164/256\n",
      "1s - loss: 0.0183 - fmeasure: 0.9976 - val_loss: 0.0103 - val_fmeasure: 0.9976\n",
      "Epoch 165/256\n",
      "1s - loss: 0.0181 - fmeasure: 0.9977 - val_loss: 0.0101 - val_fmeasure: 0.9977\n",
      "Epoch 166/256\n",
      "1s - loss: 0.0181 - fmeasure: 0.9977 - val_loss: 0.0104 - val_fmeasure: 0.9975\n",
      "Epoch 167/256\n",
      "1s - loss: 0.0182 - fmeasure: 0.9977 - val_loss: 0.0104 - val_fmeasure: 0.9978\n",
      "Epoch 168/256\n",
      "1s - loss: 0.0183 - fmeasure: 0.9976 - val_loss: 0.0107 - val_fmeasure: 0.9975\n",
      "Epoch 169/256\n",
      "1s - loss: 0.0183 - fmeasure: 0.9976 - val_loss: 0.0100 - val_fmeasure: 0.9979\n",
      "Epoch 170/256\n",
      "1s - loss: 0.0182 - fmeasure: 0.9976 - val_loss: 0.0105 - val_fmeasure: 0.9971\n",
      "Epoch 171/256\n",
      "1s - loss: 0.0181 - fmeasure: 0.9977 - val_loss: 0.0109 - val_fmeasure: 0.9974\n",
      "Epoch 172/256\n",
      "1s - loss: 0.0181 - fmeasure: 0.9977 - val_loss: 0.0107 - val_fmeasure: 0.9973\n",
      "Epoch 173/256\n",
      "1s - loss: 0.0181 - fmeasure: 0.9977 - val_loss: 0.0110 - val_fmeasure: 0.9970\n",
      "Epoch 174/256\n",
      "1s - loss: 0.0181 - fmeasure: 0.9976 - val_loss: 0.0106 - val_fmeasure: 0.9977\n",
      "Epoch 175/256\n",
      "1s - loss: 0.0181 - fmeasure: 0.9977 - val_loss: 0.0105 - val_fmeasure: 0.9977\n",
      "Epoch 176/256\n",
      "1s - loss: 0.0181 - fmeasure: 0.9976 - val_loss: 0.0101 - val_fmeasure: 0.9977\n",
      "Epoch 177/256\n",
      "1s - loss: 0.0179 - fmeasure: 0.9977 - val_loss: 0.0099 - val_fmeasure: 0.9977\n",
      "Epoch 178/256\n",
      "1s - loss: 0.0180 - fmeasure: 0.9976 - val_loss: 0.0103 - val_fmeasure: 0.9977\n",
      "Epoch 179/256\n",
      "1s - loss: 0.0180 - fmeasure: 0.9977 - val_loss: 0.0112 - val_fmeasure: 0.9975\n",
      "Epoch 180/256\n",
      "1s - loss: 0.0180 - fmeasure: 0.9976 - val_loss: 0.0115 - val_fmeasure: 0.9966\n",
      "Epoch 181/256\n",
      "1s - loss: 0.0179 - fmeasure: 0.9977 - val_loss: 0.0102 - val_fmeasure: 0.9976\n",
      "Epoch 182/256\n",
      "1s - loss: 0.0179 - fmeasure: 0.9977 - val_loss: 0.0102 - val_fmeasure: 0.9976\n",
      "Epoch 183/256\n",
      "1s - loss: 0.0179 - fmeasure: 0.9976 - val_loss: 0.0103 - val_fmeasure: 0.9973\n",
      "Epoch 184/256\n",
      "1s - loss: 0.0179 - fmeasure: 0.9978 - val_loss: 0.0104 - val_fmeasure: 0.9978\n",
      "Epoch 185/256\n",
      "1s - loss: 0.0180 - fmeasure: 0.9977 - val_loss: 0.0103 - val_fmeasure: 0.9978\n",
      "Epoch 186/256\n",
      "1s - loss: 0.0179 - fmeasure: 0.9978 - val_loss: 0.0103 - val_fmeasure: 0.9976\n",
      "Epoch 187/256\n",
      "1s - loss: 0.0178 - fmeasure: 0.9978 - val_loss: 0.0101 - val_fmeasure: 0.9977\n",
      "Epoch 188/256\n",
      "1s - loss: 0.0180 - fmeasure: 0.9977 - val_loss: 0.0099 - val_fmeasure: 0.9978\n",
      "Epoch 189/256\n",
      "1s - loss: 0.0179 - fmeasure: 0.9977 - val_loss: 0.0112 - val_fmeasure: 0.9972\n",
      "Epoch 190/256\n",
      "1s - loss: 0.0178 - fmeasure: 0.9977 - val_loss: 0.0107 - val_fmeasure: 0.9977\n",
      "Epoch 191/256\n",
      "1s - loss: 0.0178 - fmeasure: 0.9978 - val_loss: 0.0115 - val_fmeasure: 0.9971\n",
      "Epoch 192/256\n",
      "1s - loss: 0.0177 - fmeasure: 0.9977 - val_loss: 0.0098 - val_fmeasure: 0.9979\n",
      "Epoch 193/256\n",
      "1s - loss: 0.0179 - fmeasure: 0.9977 - val_loss: 0.0111 - val_fmeasure: 0.9970\n",
      "Epoch 194/256\n",
      "1s - loss: 0.0180 - fmeasure: 0.9977 - val_loss: 0.0099 - val_fmeasure: 0.9980\n",
      "Epoch 195/256\n",
      "1s - loss: 0.0177 - fmeasure: 0.9978 - val_loss: 0.0105 - val_fmeasure: 0.9971\n",
      "Epoch 196/256\n",
      "1s - loss: 0.0178 - fmeasure: 0.9977 - val_loss: 0.0100 - val_fmeasure: 0.9977\n",
      "Epoch 197/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9978 - val_loss: 0.0100 - val_fmeasure: 0.9976\n",
      "Epoch 198/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9978 - val_loss: 0.0098 - val_fmeasure: 0.9979\n",
      "Epoch 199/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9978 - val_loss: 0.0099 - val_fmeasure: 0.9978\n",
      "Epoch 200/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9977 - val_loss: 0.0103 - val_fmeasure: 0.9974\n",
      "Epoch 201/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9977 - val_loss: 0.0108 - val_fmeasure: 0.9970\n",
      "Epoch 202/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9978 - val_loss: 0.0101 - val_fmeasure: 0.9976\n",
      "Epoch 203/256\n",
      "1s - loss: 0.0175 - fmeasure: 0.9978 - val_loss: 0.0107 - val_fmeasure: 0.9972\n",
      "Epoch 204/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9978 - val_loss: 0.0106 - val_fmeasure: 0.9978\n",
      "Epoch 205/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9977 - val_loss: 0.0100 - val_fmeasure: 0.9977\n",
      "Epoch 206/256\n",
      "1s - loss: 0.0175 - fmeasure: 0.9978 - val_loss: 0.0107 - val_fmeasure: 0.9976\n",
      "Epoch 207/256\n",
      "1s - loss: 0.0175 - fmeasure: 0.9978 - val_loss: 0.0105 - val_fmeasure: 0.9974\n",
      "Epoch 208/256\n",
      "1s - loss: 0.0175 - fmeasure: 0.9978 - val_loss: 0.0103 - val_fmeasure: 0.9980\n",
      "Epoch 209/256\n",
      "1s - loss: 0.0175 - fmeasure: 0.9978 - val_loss: 0.0101 - val_fmeasure: 0.9977\n",
      "Epoch 210/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9978 - val_loss: 0.0102 - val_fmeasure: 0.9977\n",
      "Epoch 211/256\n",
      "1s - loss: 0.0176 - fmeasure: 0.9978 - val_loss: 0.0102 - val_fmeasure: 0.9977\n",
      "Epoch 212/256\n",
      "1s - loss: 0.0174 - fmeasure: 0.9978 - val_loss: 0.0107 - val_fmeasure: 0.9979\n",
      "Epoch 213/256\n",
      "1s - loss: 0.0174 - fmeasure: 0.9978 - val_loss: 0.0102 - val_fmeasure: 0.9975\n",
      "Epoch 214/256\n",
      "1s - loss: 0.0175 - fmeasure: 0.9978 - val_loss: 0.0103 - val_fmeasure: 0.9978\n",
      "Epoch 215/256\n",
      "1s - loss: 0.0174 - fmeasure: 0.9978 - val_loss: 0.0101 - val_fmeasure: 0.9978\n",
      "Epoch 216/256\n",
      "1s - loss: 0.0174 - fmeasure: 0.9978 - val_loss: 0.0103 - val_fmeasure: 0.9976\n",
      "Epoch 217/256\n",
      "1s - loss: 0.0174 - fmeasure: 0.9978 - val_loss: 0.0106 - val_fmeasure: 0.9975\n",
      "Epoch 218/256\n",
      "1s - loss: 0.0174 - fmeasure: 0.9977 - val_loss: 0.0103 - val_fmeasure: 0.9977\n",
      "Epoch 219/256\n",
      "1s - loss: 0.0173 - fmeasure: 0.9978 - val_loss: 0.0097 - val_fmeasure: 0.9978\n",
      "Epoch 220/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9978 - val_loss: 0.0099 - val_fmeasure: 0.9977\n",
      "Epoch 221/256\n",
      "1s - loss: 0.0173 - fmeasure: 0.9979 - val_loss: 0.0102 - val_fmeasure: 0.9975\n",
      "Epoch 222/256\n",
      "1s - loss: 0.0174 - fmeasure: 0.9977 - val_loss: 0.0103 - val_fmeasure: 0.9976\n",
      "Epoch 223/256\n",
      "1s - loss: 0.0173 - fmeasure: 0.9978 - val_loss: 0.0103 - val_fmeasure: 0.9975\n",
      "Epoch 224/256\n",
      "1s - loss: 0.0174 - fmeasure: 0.9977 - val_loss: 0.0109 - val_fmeasure: 0.9972\n",
      "Epoch 225/256\n",
      "1s - loss: 0.0174 - fmeasure: 0.9978 - val_loss: 0.0100 - val_fmeasure: 0.9977\n",
      "Epoch 226/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9978 - val_loss: 0.0098 - val_fmeasure: 0.9977\n",
      "Epoch 227/256\n",
      "1s - loss: 0.0173 - fmeasure: 0.9978 - val_loss: 0.0097 - val_fmeasure: 0.9980\n",
      "Epoch 228/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9978 - val_loss: 0.0106 - val_fmeasure: 0.9973\n",
      "Epoch 229/256\n",
      "1s - loss: 0.0173 - fmeasure: 0.9979 - val_loss: 0.0099 - val_fmeasure: 0.9979\n",
      "Epoch 230/256\n",
      "1s - loss: 0.0173 - fmeasure: 0.9979 - val_loss: 0.0101 - val_fmeasure: 0.9975\n",
      "Epoch 231/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9978 - val_loss: 0.0102 - val_fmeasure: 0.9977\n",
      "Epoch 232/256\n",
      "1s - loss: 0.0171 - fmeasure: 0.9979 - val_loss: 0.0100 - val_fmeasure: 0.9976\n",
      "Epoch 233/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9979 - val_loss: 0.0099 - val_fmeasure: 0.9977\n",
      "Epoch 234/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9979 - val_loss: 0.0101 - val_fmeasure: 0.9974\n",
      "Epoch 235/256\n",
      "1s - loss: 0.0171 - fmeasure: 0.9978 - val_loss: 0.0097 - val_fmeasure: 0.9979\n",
      "Epoch 236/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9978 - val_loss: 0.0102 - val_fmeasure: 0.9977\n",
      "Epoch 237/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9979 - val_loss: 0.0098 - val_fmeasure: 0.9981\n",
      "Epoch 238/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9979 - val_loss: 0.0101 - val_fmeasure: 0.9977\n",
      "Epoch 239/256\n",
      "1s - loss: 0.0170 - fmeasure: 0.9979 - val_loss: 0.0101 - val_fmeasure: 0.9978\n",
      "Epoch 240/256\n",
      "1s - loss: 0.0170 - fmeasure: 0.9980 - val_loss: 0.0102 - val_fmeasure: 0.9975\n",
      "Epoch 241/256\n",
      "1s - loss: 0.0172 - fmeasure: 0.9977 - val_loss: 0.0100 - val_fmeasure: 0.9977\n",
      "Epoch 242/256\n",
      "1s - loss: 0.0170 - fmeasure: 0.9979 - val_loss: 0.0103 - val_fmeasure: 0.9973\n",
      "Epoch 243/256\n",
      "1s - loss: 0.0171 - fmeasure: 0.9978 - val_loss: 0.0103 - val_fmeasure: 0.9971\n",
      "Epoch 244/256\n",
      "1s - loss: 0.0170 - fmeasure: 0.9979 - val_loss: 0.0099 - val_fmeasure: 0.9978\n",
      "Epoch 245/256\n",
      "1s - loss: 0.0171 - fmeasure: 0.9978 - val_loss: 0.0104 - val_fmeasure: 0.9972\n",
      "Epoch 246/256\n",
      "1s - loss: 0.0170 - fmeasure: 0.9978 - val_loss: 0.0099 - val_fmeasure: 0.9979\n",
      "Epoch 247/256\n",
      "1s - loss: 0.0169 - fmeasure: 0.9979 - val_loss: 0.0097 - val_fmeasure: 0.9982\n",
      "Epoch 248/256\n",
      "1s - loss: 0.0170 - fmeasure: 0.9978 - val_loss: 0.0097 - val_fmeasure: 0.9978\n",
      "Epoch 249/256\n",
      "1s - loss: 0.0170 - fmeasure: 0.9979 - val_loss: 0.0099 - val_fmeasure: 0.9978\n",
      "Epoch 250/256\n",
      "1s - loss: 0.0169 - fmeasure: 0.9979 - val_loss: 0.0098 - val_fmeasure: 0.9978\n",
      "Epoch 251/256\n",
      "1s - loss: 0.0170 - fmeasure: 0.9979 - val_loss: 0.0105 - val_fmeasure: 0.9974\n",
      "Epoch 252/256\n",
      "1s - loss: 0.0169 - fmeasure: 0.9978 - val_loss: 0.0248 - val_fmeasure: 0.9919\n",
      "Epoch 253/256\n",
      "1s - loss: 0.0175 - fmeasure: 0.9978 - val_loss: 0.0102 - val_fmeasure: 0.9974\n",
      "Epoch 254/256\n",
      "1s - loss: 0.0170 - fmeasure: 0.9979 - val_loss: 0.0097 - val_fmeasure: 0.9978\n",
      "Epoch 255/256\n",
      "1s - loss: 0.0169 - fmeasure: 0.9980 - val_loss: 0.0102 - val_fmeasure: 0.9976\n",
      "Epoch 256/256\n",
      "1s - loss: 0.0169 - fmeasure: 0.9979 - val_loss: 0.0103 - val_fmeasure: 0.9975\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, nb_epoch = 256, batch_size = 750, verbose = 2, \n",
    "                    validation_split = .20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our neural network achieves an f1 score of  0.997443272611 on the oversampled testing data\n",
      "[[53057   240]\n",
      " [   32 53286]]\n",
      "Our neural network achieves an f1 score of 0.997765733255 on the original testing data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[70782,   303],\n",
       "       [   14,   103]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions[:] = predictions[:]>0.5\n",
    "\n",
    "print \"Our neural network achieves an f1 score of \" , f1_score(y_test, predictions, pos_label = 0) , \"on the oversampled testing data\"\n",
    "print confusion_matrix(y_test, predictions)\n",
    "\n",
    "opredictions = model.predict(test_features.values)\n",
    "opredictions[:] = opredictions[:]>0.5\n",
    "print \"Our neural network achieves an f1 score of\" , f1_score(test_labels.values, opredictions, pos_label = 0) , \"on the original testing data\"\n",
    "confusion_matrix(test_labels.values, opredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEZCAYAAAC5AHPcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQJAiwABoEWAANDyDVjhaX+lK9LEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2abafef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('binary crossentropy loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training loss', 'validation loss'], loc = 'upper right')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
